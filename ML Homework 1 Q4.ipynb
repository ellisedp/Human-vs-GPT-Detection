{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5accd7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/Ellise/opt/anaconda3/lib/python3.9/site-packages (4.64.0)\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d4301",
   "metadata": {},
   "source": [
    "## Read data and combine both human and GPT dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b64c8eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of dataframe containing human generated documents: (39713, 2)\n",
      "Dimensions of dataframe containing gpt generated documents: (20398, 2)\n",
      "Dimensions of dataframe containing both document classes: (60111, 2)\n"
     ]
    }
   ],
   "source": [
    "def read_data(file_path, author):\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line_number, line_content in enumerate(file, start=1):\n",
    "            row_data = []\n",
    "            row_data.append('<START> ')\n",
    "            for char in line_content.strip():\n",
    "                # Remove unwanted chars\n",
    "                if char == '.' or char == ',' or char == '!' or char == '?' or (char.isalpha() or char.isdigit() or char.isspace()):\n",
    "                    row_data.append(char)\n",
    "            row_data.append(' <END>')\n",
    "            row_data = [element.lower() for element in row_data]\n",
    "            \n",
    "            # To ensure each row only has one column\n",
    "            if len(row_data) > 0:\n",
    "                data.append([''.join(row_data), author])\n",
    "\n",
    "    return pd.DataFrame(data, columns=['Content', 'Author'])\n",
    "\n",
    "# To view whole line\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Test\n",
    "hum = read_data(\"hum.txt\", 'Human')\n",
    "print(\"Dimensions of dataframe containing human generated documents:\", hum.shape)\n",
    "\n",
    "gpt = read_data(\"gpt.txt\", 'GPT')\n",
    "print(\"Dimensions of dataframe containing gpt generated documents:\", gpt.shape)\n",
    "\n",
    "df = pd.concat([hum, gpt], ignore_index = True)\n",
    "print(\"Dimensions of dataframe containing both document classes:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4f2fad",
   "metadata": {},
   "source": [
    "## Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8744236d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of training dataframe: (54100, 2)\n",
      "Dimensions of testing dataframe: (6011, 2)\n"
     ]
    }
   ],
   "source": [
    "hum_train_amount = round(0.9 * hum.shape[0])\n",
    "gpt_train_amount = round(0.9 * gpt.shape[0])\n",
    "\n",
    "# Create training and testing dataframes\n",
    "columns = ['Content', 'Author']\n",
    "train = pd.DataFrame(columns = columns)\n",
    "test = pd.DataFrame(columns = columns)\n",
    "train_hum = pd.DataFrame(columns = columns)\n",
    "test_hum = pd.DataFrame(columns = columns)\n",
    "train_gpt = pd.DataFrame(columns = columns)\n",
    "test_gpt = pd.DataFrame(columns = columns)\n",
    "\n",
    "train_hum = hum.iloc[:hum_train_amount]\n",
    "train_gpt = gpt.iloc[:gpt_train_amount]\n",
    "\n",
    "test_hum = hum.iloc[hum_train_amount:].reset_index(drop=True)\n",
    "test_gpt = gpt.iloc[gpt_train_amount:].reset_index(drop=True)\n",
    "\n",
    "train = pd.concat([train_hum, train_gpt], ignore_index=True)\n",
    "test = pd.concat([test_hum, test_gpt], ignore_index=True)\n",
    "\n",
    "print(\"Dimensions of training dataframe:\", train.shape)\n",
    "print(\"Dimensions of testing dataframe:\", test.shape)\n",
    "\n",
    "#print(test_hum['Content'][0])\n",
    "#print(test_hum.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493bb20e",
   "metadata": {},
   "source": [
    "## Get n-grams per class in the training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b2edca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find ngrams\n",
    "def find_ngrams(words, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngram = tuple(words[i : i + n])\n",
    "        if ngram != ('<end>', '<start>'):  # Relationship between documents (?)\n",
    "            ngrams.append(ngram)\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce77010f",
   "metadata": {},
   "source": [
    "Find bi/trigrams per document in human and gpt training sets \n",
    "- train_bigrams_hum_dict \n",
    "- train_trigrams_hum_dict\n",
    "- train_bigrams_gpt_dict\n",
    "- train_trigrams_gpt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deadfcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_trigrams_gpt_dict:\n",
      "[(0, [('<start>', '<start>', 'when'), ('<start>', 'when', 'a'), ('when', 'a', 'breeze'), ('a', 'breeze', 'blows'), ('breeze', 'blows', 'on'), ('blows', 'on', 'your'), ('on', 'your', 'skin'), ('your', 'skin', ','), ('skin', ',', 'it'), (',', 'it', 'can'), ('it', 'can', 'help'), ('can', 'help', 'to'), ('help', 'to', 'evaporate'), ('to', 'evaporate', 'sweat'), ('evaporate', 'sweat', 'from'), ('sweat', 'from', 'your'), ('from', 'your', 'body'), ('your', 'body', '.'), ('body', '.', 'this'), ('.', 'this', 'process'), ('this', 'process', 'of'), ('process', 'of', 'evaporation'), ('of', 'evaporation', 'cools'), ('evaporation', 'cools', 'your'), ('cools', 'your', 'skin'), ('your', 'skin', ','), ('skin', ',', 'which'), (',', 'which', 'is'), ('which', 'is', 'why'), ('is', 'why', 'a'), ('why', 'a', 'breeze'), ('a', 'breeze', 'can'), ('breeze', 'can', 'feel'), ('can', 'feel', 'cool'), ('feel', 'cool', ','), ('cool', ',', 'even'), (',', 'even', 'if'), ('even', 'if', 'the'), ('if', 'the', 'air'), ('the', 'air', 'itself'), ('air', 'itself', 'is'), ('itself', 'is', 'hot'), ('is', 'hot', '.'), ('hot', '.', 'humidity'), ('.', 'humidity', 'can'), ('humidity', 'can', 'affect'), ('can', 'affect', 'how'), ('affect', 'how', 'cool'), ('how', 'cool', 'a'), ('cool', 'a', 'breeze'), ('a', 'breeze', 'feels'), ('breeze', 'feels', 'because'), ('feels', 'because', 'high'), ('because', 'high', 'humidity'), ('high', 'humidity', 'means'), ('humidity', 'means', 'that'), ('means', 'that', 'there'), ('that', 'there', 'is'), ('there', 'is', 'a'), ('is', 'a', 'lot'), ('a', 'lot', 'of'), ('lot', 'of', 'moisture'), ('of', 'moisture', 'in'), ('moisture', 'in', 'the'), ('in', 'the', 'air'), ('the', 'air', 'already'), ('air', 'already', '.'), ('already', '.', 'when'), ('.', 'when', 'the'), ('when', 'the', 'air'), ('the', 'air', 'is'), ('air', 'is', 'already'), ('is', 'already', 'very'), ('already', 'very', 'moist'), ('very', 'moist', ','), ('moist', ',', 'it'), (',', 'it', 'is'), ('it', 'is', 'harder'), ('is', 'harder', 'for'), ('harder', 'for', 'sweat'), ('for', 'sweat', 'to'), ('sweat', 'to', 'evaporate'), ('to', 'evaporate', 'from'), ('evaporate', 'from', 'your'), ('from', 'your', 'skin'), ('your', 'skin', ','), ('skin', ',', 'which'), (',', 'which', 'means'), ('which', 'means', 'that'), ('means', 'that', 'the'), ('that', 'the', 'breeze'), ('the', 'breeze', 'will'), ('breeze', 'will', 'not'), ('will', 'not', 'be'), ('not', 'be', 'as'), ('be', 'as', 'effective'), ('as', 'effective', 'at'), ('effective', 'at', 'cooling'), ('at', 'cooling', 'you'), ('cooling', 'you', 'down'), ('you', 'down', '.'), ('down', '.', 'on'), ('.', 'on', 'the'), ('on', 'the', 'other'), ('the', 'other', 'hand'), ('other', 'hand', ','), ('hand', ',', 'if'), (',', 'if', 'the'), ('if', 'the', 'air'), ('the', 'air', 'is'), ('air', 'is', 'dry'), ('is', 'dry', ','), ('dry', ',', 'it'), (',', 'it', 'is'), ('it', 'is', 'easier'), ('is', 'easier', 'for'), ('easier', 'for', 'sweat'), ('for', 'sweat', 'to'), ('sweat', 'to', 'evaporate'), ('to', 'evaporate', ','), ('evaporate', ',', 'so'), (',', 'so', 'the'), ('so', 'the', 'breeze'), ('the', 'breeze', 'will'), ('breeze', 'will', 'feel'), ('will', 'feel', 'cooler'), ('feel', 'cooler', '.'), ('cooler', '.', '<end>')]), (1, [('<start>', '<start>', 'cats'), ('<start>', 'cats', 'have'), ('cats', 'have', 'scent'), ('have', 'scent', 'glands'), ('scent', 'glands', 'on'), ('glands', 'on', 'their'), ('on', 'their', 'faces'), ('their', 'faces', ','), ('faces', ',', 'especially'), (',', 'especially', 'on'), ('especially', 'on', 'their'), ('on', 'their', 'chins'), ('their', 'chins', 'and'), ('chins', 'and', 'around'), ('and', 'around', 'their'), ('around', 'their', 'cheeks'), ('their', 'cheeks', '.'), ('cheeks', '.', 'when'), ('.', 'when', 'they'), ('when', 'they', 'rub'), ('they', 'rub', 'their'), ('rub', 'their', 'faces'), ('their', 'faces', 'on'), ('faces', 'on', 'things'), ('on', 'things', ','), ('things', ',', 'they'), (',', 'they', 'are'), ('they', 'are', 'marking'), ('are', 'marking', 'their'), ('marking', 'their', 'territory'), ('their', 'territory', 'with'), ('territory', 'with', 'their'), ('with', 'their', 'scent'), ('their', 'scent', '.'), ('scent', '.', 'this'), ('.', 'this', 'is'), ('this', 'is', 'a'), ('is', 'a', 'way'), ('a', 'way', 'for'), ('way', 'for', 'them'), ('for', 'them', 'to'), ('them', 'to', 'claim'), ('to', 'claim', 'objects'), ('claim', 'objects', ','), ('objects', ',', 'people'), (',', 'people', ','), ('people', ',', 'and'), (',', 'and', 'even'), ('and', 'even', 'areas'), ('even', 'areas', 'as'), ('areas', 'as', 'their'), ('as', 'their', 'own'), ('their', 'own', '.'), ('own', '.', 'cats'), ('.', 'cats', 'may'), ('cats', 'may', 'also'), ('may', 'also', 'do'), ('also', 'do', 'this'), ('do', 'this', 'as'), ('this', 'as', 'a'), ('as', 'a', 'way'), ('a', 'way', 'to'), ('way', 'to', 'show'), ('to', 'show', 'affection'), ('show', 'affection', 'to'), ('affection', 'to', 'their'), ('to', 'their', 'human'), ('their', 'human', 'companions'), ('human', 'companions', 'or'), ('companions', 'or', 'other'), ('or', 'other', 'animals'), ('other', 'animals', '.'), ('animals', '.', 'when'), ('.', 'when', 'they'), ('when', 'they', 'rub'), ('they', 'rub', 'against'), ('rub', 'against', 'you'), ('against', 'you', ','), ('you', ',', 'they'), (',', 'they', 'are'), ('they', 'are', 'marking'), ('are', 'marking', 'you'), ('marking', 'you', 'with'), ('you', 'with', 'their'), ('with', 'their', 'scent'), ('their', 'scent', 'and'), ('scent', 'and', 'showing'), ('and', 'showing', 'that'), ('showing', 'that', 'you'), ('that', 'you', 'are'), ('you', 'are', 'a'), ('are', 'a', 'part'), ('a', 'part', 'of'), ('part', 'of', 'their'), ('of', 'their', 'social'), ('their', 'social', 'group'), ('social', 'group', '.'), ('group', '.', 'rubbing'), ('.', 'rubbing', 'their'), ('rubbing', 'their', 'faces'), ('their', 'faces', 'on'), ('faces', 'on', 'you'), ('on', 'you', 'can'), ('you', 'can', 'also'), ('can', 'also', 'be'), ('also', 'be', 'a'), ('be', 'a', 'way'), ('a', 'way', 'for'), ('way', 'for', 'cats'), ('for', 'cats', 'to'), ('cats', 'to', 'communicate'), ('to', 'communicate', 'their'), ('communicate', 'their', 'needs'), ('their', 'needs', ','), ('needs', ',', 'such'), (',', 'such', 'as'), ('such', 'as', 'wanting'), ('as', 'wanting', 'food'), ('wanting', 'food', 'or'), ('food', 'or', 'attention'), ('or', 'attention', '.'), ('attention', '.', 'its'), ('.', 'its', 'a'), ('its', 'a', 'way'), ('a', 'way', 'for'), ('way', 'for', 'them'), ('for', 'them', 'to'), ('them', 'to', 'get'), ('to', 'get', 'your'), ('get', 'your', 'attention'), ('your', 'attention', 'and'), ('attention', 'and', 'let'), ('and', 'let', 'you'), ('let', 'you', 'know'), ('you', 'know', 'what'), ('know', 'what', 'they'), ('what', 'they', 'want'), ('they', 'want', '.'), ('want', '.', 'overall'), ('.', 'overall', ','), ('overall', ',', 'rubbing'), (',', 'rubbing', 'their'), ('rubbing', 'their', 'faces'), ('their', 'faces', 'on'), ('faces', 'on', 'things'), ('on', 'things', 'is'), ('things', 'is', 'just'), ('is', 'just', 'one'), ('just', 'one', 'of'), ('one', 'of', 'the'), ('of', 'the', 'many'), ('the', 'many', 'ways'), ('many', 'ways', 'that'), ('ways', 'that', 'cats'), ('that', 'cats', 'communicate'), ('cats', 'communicate', 'and'), ('communicate', 'and', 'interact'), ('and', 'interact', 'with'), ('interact', 'with', 'their'), ('with', 'their', 'environment'), ('their', 'environment', 'and'), ('environment', 'and', 'the'), ('and', 'the', 'people'), ('the', 'people', 'and'), ('people', 'and', 'animals'), ('and', 'animals', 'around'), ('animals', 'around', 'them'), ('around', 'them', '.'), ('them', '.', '<end>')]), (2, [('<start>', '<start>', 'before'), ('<start>', 'before', 'the'), ('before', 'the', 'widespread'), ('the', 'widespread', 'use'), ('widespread', 'use', 'of'), ('use', 'of', 'the'), ('of', 'the', 'internet'), ('the', 'internet', ','), ('internet', ',', 'people'), (',', 'people', 'typically'), ('people', 'typically', 'purchased'), ('typically', 'purchased', 'airline'), ('purchased', 'airline', 'tickets'), ('airline', 'tickets', 'in'), ('tickets', 'in', 'one'), ('in', 'one', 'of'), ('one', 'of', 'several'), ('of', 'several', 'ways'), ('several', 'ways', '.'), ('ways', '.', 'here'), ('.', 'here', 'are'), ('here', 'are', 'a'), ('are', 'a', 'few'), ('a', 'few', 'examples'), ('few', 'examples', 'by'), ('examples', 'by', 'phone'), ('by', 'phone', 'many'), ('phone', 'many', 'airlines'), ('many', 'airlines', 'had'), ('airlines', 'had', 'a'), ('had', 'a', 'phone'), ('a', 'phone', 'number'), ('phone', 'number', 'that'), ('number', 'that', 'you'), ('that', 'you', 'could'), ('you', 'could', 'call'), ('could', 'call', 'to'), ('call', 'to', 'book'), ('to', 'book', 'a'), ('book', 'a', 'flight'), ('a', 'flight', '.'), ('flight', '.', 'you'), ('.', 'you', 'would'), ('you', 'would', 'provide'), ('would', 'provide', 'the'), ('provide', 'the', 'necessary'), ('the', 'necessary', 'information'), ('necessary', 'information', 'such'), ('information', 'such', 'as'), ('such', 'as', 'your'), ('as', 'your', 'destination'), ('your', 'destination', ','), ('destination', ',', 'the'), (',', 'the', 'dates'), ('the', 'dates', 'you'), ('dates', 'you', 'wanted'), ('you', 'wanted', 'to'), ('wanted', 'to', 'travel'), ('to', 'travel', ','), ('travel', ',', 'and'), (',', 'and', 'the'), ('and', 'the', 'number'), ('the', 'number', 'of'), ('number', 'of', 'tickets'), ('of', 'tickets', 'you'), ('tickets', 'you', 'needed'), ('you', 'needed', ','), ('needed', ',', 'and'), (',', 'and', 'a'), ('and', 'a', 'representative'), ('a', 'representative', 'would'), ('representative', 'would', 'help'), ('would', 'help', 'you'), ('help', 'you', 'book'), ('you', 'book', 'the'), ('book', 'the', 'flight'), ('the', 'flight', '.'), ('flight', '.', 'in'), ('.', 'in', 'person'), ('in', 'person', 'you'), ('person', 'you', 'could'), ('you', 'could', 'also'), ('could', 'also', 'go'), ('also', 'go', 'to'), ('go', 'to', 'the'), ('to', 'the', 'airport'), ('the', 'airport', 'or'), ('airport', 'or', 'a'), ('or', 'a', 'travel'), ('a', 'travel', 'agency'), ('travel', 'agency', 'to'), ('agency', 'to', 'purchase'), ('to', 'purchase', 'a'), ('purchase', 'a', 'ticket'), ('a', 'ticket', 'in'), ('ticket', 'in', 'person'), ('in', 'person', '.'), ('person', '.', 'this'), ('.', 'this', 'might'), ('this', 'might', 'be'), ('might', 'be', 'a'), ('be', 'a', 'good'), ('a', 'good', 'option'), ('good', 'option', 'if'), ('option', 'if', 'you'), ('if', 'you', 'wanted'), ('you', 'wanted', 'to'), ('wanted', 'to', 'ask'), ('to', 'ask', 'questions'), ('ask', 'questions', 'or'), ('questions', 'or', 'get'), ('or', 'get', 'help'), ('get', 'help', 'with'), ('help', 'with', 'your'), ('with', 'your', 'travel'), ('your', 'travel', 'plans'), ('travel', 'plans', '.'), ('plans', '.', 'by'), ('.', 'by', 'mail'), ('by', 'mail', 'some'), ('mail', 'some', 'airlines'), ('some', 'airlines', 'allowed'), ('airlines', 'allowed', 'you'), ('allowed', 'you', 'to'), ('you', 'to', 'send'), ('to', 'send', 'in'), ('send', 'in', 'a'), ('in', 'a', 'request'), ('a', 'request', 'for'), ('request', 'for', 'tickets'), ('for', 'tickets', 'by'), ('tickets', 'by', 'mail'), ('by', 'mail', '.'), ('mail', '.', 'you'), ('.', 'you', 'would'), ('you', 'would', 'fill'), ('would', 'fill', 'out'), ('fill', 'out', 'a'), ('out', 'a', 'form'), ('a', 'form', 'and'), ('form', 'and', 'send'), ('and', 'send', 'it'), ('send', 'it', 'to'), ('it', 'to', 'the'), ('to', 'the', 'airline'), ('the', 'airline', ','), ('airline', ',', 'along'), (',', 'along', 'with'), ('along', 'with', 'a'), ('with', 'a', 'check'), ('a', 'check', 'or'), ('check', 'or', 'money'), ('or', 'money', 'order'), ('money', 'order', 'to'), ('order', 'to', 'pay'), ('to', 'pay', 'for'), ('pay', 'for', 'the'), ('for', 'the', 'tickets'), ('the', 'tickets', '.'), ('tickets', '.', 'overall'), ('.', 'overall', ','), ('overall', ',', 'it'), (',', 'it', 'was'), ('it', 'was', 'usually'), ('was', 'usually', 'possible'), ('usually', 'possible', 'to'), ('possible', 'to', 'purchase'), ('to', 'purchase', 'tickets'), ('purchase', 'tickets', 'in'), ('tickets', 'in', 'advance'), ('in', 'advance', 'for'), ('advance', 'for', 'future'), ('for', 'future', 'travel'), ('future', 'travel', ','), ('travel', ',', 'although'), (',', 'although', 'you'), ('although', 'you', 'might'), ('you', 'might', 'have'), ('might', 'have', 'had'), ('have', 'had', 'to'), ('had', 'to', 'plan'), ('to', 'plan', 'ahead'), ('plan', 'ahead', 'if'), ('ahead', 'if', 'you'), ('if', 'you', 'wanted'), ('you', 'wanted', 'to'), ('wanted', 'to', 'book'), ('to', 'book', 'a'), ('book', 'a', 'flight'), ('a', 'flight', 'that'), ('flight', 'that', 'was'), ('that', 'was', 'very'), ('was', 'very', 'far'), ('very', 'far', 'in'), ('far', 'in', 'the'), ('in', 'the', 'future'), ('the', 'future', 'such'), ('future', 'such', 'as'), ('such', 'as', '23'), ('as', '23', 'months'), ('23', 'months', '.'), ('months', '.', '<end>')])]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize data \n",
    "train_words_hum = train_hum.copy()\n",
    "train_words_hum['Content'] = train_words_hum['Content'].apply(lambda x: x.split())\n",
    "train_words_gpt = train_gpt.copy()\n",
    "train_words_gpt['Content'] = train_words_gpt['Content'].apply(lambda x: x.split())\n",
    "\n",
    "# Find n-grams \n",
    "train_bigrams_hum = train_words_hum['Content'].apply(lambda x: find_ngrams(x, 2))\n",
    "train_trigrams_hum = train_words_hum['Content'].apply(lambda x: find_ngrams(x, 3))\n",
    "train_bigrams_hum_dict = train_bigrams_hum.to_dict()\n",
    "train_trigrams_hum_dict = train_trigrams_hum.to_dict()\n",
    "\n",
    "train_bigrams_gpt = train_words_gpt['Content'].apply(lambda x: find_ngrams(x, 2))\n",
    "train_trigrams_gpt = train_words_gpt['Content'].apply(lambda x: find_ngrams(x, 3))\n",
    "train_bigrams_gpt_dict = train_bigrams_gpt.to_dict()\n",
    "train_trigrams_gpt_dict = train_trigrams_gpt.to_dict()\n",
    "\n",
    "for key, trigrams_list in train_trigrams_hum_dict.items():\n",
    "    if len(trigrams_list) >= 2:\n",
    "        second_word = trigrams_list[1][0]\n",
    "        trigrams_list.insert(0, ('<start>', '<start>', second_word))\n",
    "for key, trigrams_list in train_trigrams_gpt_dict.items():\n",
    "    if len(trigrams_list) >= 2:\n",
    "        second_word = trigrams_list[1][0]\n",
    "        trigrams_list.insert(0, ('<start>', '<start>', second_word))\n",
    "\n",
    "\n",
    "#print(\"train_trigrams_gpt_dict:\")\n",
    "#print(list(train_trigrams_gpt_dict.items())[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d9692",
   "metadata": {},
   "source": [
    "## Get n-grams per class in the testing corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd25c8cc",
   "metadata": {},
   "source": [
    "Find bi/trigrams per document in human and gpt testing sets \n",
    "- test_bigrams_hum_dict \n",
    "- test_trigrams_hum_dict\n",
    "- test_bigrams_gpt_dict\n",
    "- test_trigrams_gpt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3289e088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_trigrams_hum_dict:\n",
      "('<start>', 'it')\n"
     ]
    }
   ],
   "source": [
    "# Tokenize data \n",
    "test_words_hum = test_hum.copy()\n",
    "test_words_hum['Content'] = test_words_hum['Content'].apply(lambda x: x.split())\n",
    "test_words_gpt = test_gpt.copy()\n",
    "test_words_gpt['Content'] = test_words_gpt['Content'].apply(lambda x: x.split())\n",
    "\n",
    "# Find n-grams \n",
    "test_bigrams_hum = test_words_hum['Content'].apply(lambda x: find_ngrams(x, 2))\n",
    "test_trigrams_hum = test_words_hum['Content'].apply(lambda x: find_ngrams(x, 3))\n",
    "test_bigrams_hum_dict = test_bigrams_hum.to_dict()\n",
    "test_trigrams_hum_dict = test_trigrams_hum.to_dict()\n",
    "\n",
    "test_bigrams_gpt = test_words_gpt['Content'].apply(lambda x: find_ngrams(x, 2))\n",
    "test_trigrams_gpt = test_words_gpt['Content'].apply(lambda x: find_ngrams(x, 3))\n",
    "test_bigrams_gpt_dict = test_bigrams_gpt.to_dict()\n",
    "test_trigrams_gpt_dict = test_trigrams_gpt.to_dict()\n",
    "\n",
    "print(\"test_trigrams_hum_dict:\")\n",
    "print(list(test_trigrams_hum_dict.items())[0][1][0][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90963c95",
   "metadata": {},
   "source": [
    "## N-gram frequencies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244ee279",
   "metadata": {},
   "source": [
    "Training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4605c24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram frequencies for human training set: [('<start>', 35742), ('not', 17576), ('a', 86621), ('matter', 954), ('of', 73932)]\n",
      "Bigram frequencies for human training set: [(('<start>', 'not'), 197), (('not', 'a'), 863), (('a', 'matter'), 152), (('matter', 'of'), 159), (('of', 'muscles'), 10)]\n",
      "Trigram frequencies for human training set: [(('<start>', '<start>', 'not'), 197), (('<start>', 'not', 'a'), 16), (('not', 'a', 'matter'), 9), (('a', 'matter', 'of'), 145), (('matter', 'of', 'muscles'), 1)]\n",
      "\n",
      "Unigram frequencies for GPT training set: [('<start>', 18358), ('when', 11483), ('a', 101329), ('breeze', 10), ('blows', 28)]\n",
      "Bigram frequencies for GPT training set: [(('<start>', 'when'), 868), (('when', 'a'), 1015), (('a', 'breeze'), 3), (('breeze', 'blows'), 1), (('blows', 'on'), 2)]\n",
      "Trigram frequencies for GPT training set: [(('<start>', '<start>', 'when'), 868), (('<start>', 'when', 'a'), 139), (('when', 'a', 'breeze'), 1), (('a', 'breeze', 'blows'), 1), (('breeze', 'blows', 'on'), 1)]\n"
     ]
    }
   ],
   "source": [
    "def calculate_ngram_freq(ngrams):\n",
    "    ngram_freq = {}\n",
    "    for ngram_list in ngrams.values():\n",
    "        for ngram in ngram_list:\n",
    "            if ngram not in ngram_freq:\n",
    "                ngram_freq[ngram] = 1\n",
    "            else:\n",
    "                ngram_freq[ngram] += 1\n",
    "    return ngram_freq\n",
    "\n",
    "\n",
    "def calculate_word_freq(words): \n",
    "    word_freq = {}\n",
    "    for i in range(len(words)):\n",
    "        for word in words[i]:\n",
    "            if word not in word_freq:\n",
    "                word_freq[word] = 1\n",
    "            else:\n",
    "                word_freq[word] += 1\n",
    "    return word_freq\n",
    "\n",
    "\n",
    "# Calculate n-gram frequencies for human training set\n",
    "unigram_freq_hum = calculate_word_freq(train_words_hum[\"Content\"])\n",
    "bigram_freq_hum = calculate_ngram_freq(train_bigrams_hum_dict)\n",
    "trigram_freq_hum = calculate_ngram_freq(train_trigrams_hum_dict)\n",
    "\n",
    "# Calculate n-gram frequencies for GPT training set\n",
    "unigram_freq_gpt = calculate_word_freq(train_words_gpt[\"Content\"])\n",
    "bigram_freq_gpt = calculate_ngram_freq(train_bigrams_gpt_dict)\n",
    "trigram_freq_gpt = calculate_ngram_freq(train_trigrams_gpt_dict)\n",
    "\n",
    "print(\"Unigram frequencies for human training set:\", list(unigram_freq_hum.items())[:5])\n",
    "print(\"Bigram frequencies for human training set:\", list(bigram_freq_hum.items())[:5])\n",
    "print(\"Trigram frequencies for human training set:\", list(trigram_freq_hum.items())[:5])\n",
    "print()\n",
    "print(\"Unigram frequencies for GPT training set:\", list(unigram_freq_gpt.items())[:5])\n",
    "print(\"Bigram frequencies for GPT training set:\", list(bigram_freq_gpt.items())[:5])\n",
    "print(\"Trigram frequencies for GPT training set:\", list(trigram_freq_gpt.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b55f3ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of unigrams in the human data: 57707\n",
      "# of bigrams in the human data: 799786\n",
      "# of trigrams in the human data: 2093325\n",
      "\n",
      "# of unigrams in the gpt data: 38380\n",
      "# of bigrams in the gpt data: 471878\n",
      "# of trigrams in the gpt data: 1393225\n"
     ]
    }
   ],
   "source": [
    "print(\"# of unigrams in the human data:\", len(unigram_freq_hum))\n",
    "print(\"# of bigrams in the human data:\", len(bigram_freq_hum))\n",
    "print(\"# of trigrams in the human data:\", len(trigram_freq_hum))\n",
    "print()\n",
    "print(\"# of unigrams in the gpt data:\", len(unigram_freq_gpt))\n",
    "print(\"# of bigrams in the gpt data:\", len(bigram_freq_gpt))\n",
    "print(\"# of trigrams in the gpt data:\", len(trigram_freq_gpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19e7a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate vocabulary \n",
    "# (number of unique unigrams in the training set (including both hum and gpt))\n",
    "\n",
    "combined_unigram_dicts = {**unigram_freq_hum, **unigram_freq_gpt}\n",
    "vocab = len(combined_unigram_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3336063",
   "metadata": {},
   "source": [
    "Testing sets\n",
    "\n",
    "Calculate and report the percentage of bigrams/trigrams in the test set (counted with repeats) that do not\n",
    "appear in the training corpus (this is called the OOV rate). You should calculate two OOV rates, one for\n",
    "bigrams and one for trigrams.\n",
    "\n",
    "Compare each bigrams in the testing set with all bigrams in the training set. If they do not equal any of the bigrams in the training set, add 1 to the counter. At the end, divide this number by the total number of bigrams in the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f64610f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all of the bigrams in training and all in testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6379f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/72/9576tn712d71pv50d2f870g40000gn/T/ipykernel_9373/3018787587.py:7: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  combined_train_bigrams = train_bigrams_hum.append(train_bigrams_gpt, ignore_index=True)\n",
      "/var/folders/72/9576tn712d71pv50d2f870g40000gn/T/ipykernel_9373/3018787587.py:8: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  combined_train_trigrams = train_trigrams_hum.append(train_trigrams_gpt, ignore_index=True)\n",
      "/var/folders/72/9576tn712d71pv50d2f870g40000gn/T/ipykernel_9373/3018787587.py:10: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  combined_test_bigrams = test_bigrams_hum.append(test_bigrams_gpt, ignore_index=True)\n",
      "/var/folders/72/9576tn712d71pv50d2f870g40000gn/T/ipykernel_9373/3018787587.py:11: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  combined_test_trigrams = test_trigrams_hum.append(test_trigrams_gpt, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined OOV rate for bigrams: 0.30536979636477\n",
      "Combined OOV rate for trigrams: 0.6719696959942729\n"
     ]
    }
   ],
   "source": [
    "def get_oov(test_ngrams, train_ngrams):\n",
    "    not_in_train = [ngram for ngrams_list in test_ngrams for ngram in ngrams_list if ngram not in train_ngrams]\n",
    "    oov_rate = len(set(not_in_train)) / sum(len(ngrams_list) for ngrams_list in test_ngrams) \n",
    "    return oov_rate\n",
    "\n",
    "# Combine human and GPT n-grams sets\n",
    "combined_train_bigrams = train_bigrams_hum.append(train_bigrams_gpt, ignore_index=True)\n",
    "combined_train_trigrams = train_trigrams_hum.append(train_trigrams_gpt, ignore_index=True)\n",
    "\n",
    "combined_test_bigrams = test_bigrams_hum.append(test_bigrams_gpt, ignore_index=True)\n",
    "combined_test_trigrams = test_trigrams_hum.append(test_trigrams_gpt, ignore_index=True)\n",
    "\n",
    "# Calculate OOV rates\n",
    "oov_rate_combined_bigrams = get_oov(combined_test_bigrams, combined_train_bigrams)\n",
    "oov_rate_combined_trigrams = get_oov(combined_test_trigrams, combined_train_trigrams)\n",
    "\n",
    "print(\"Combined OOV rate for bigrams:\", oov_rate_combined_bigrams)\n",
    "print(\"Combined OOV rate for trigrams:\", oov_rate_combined_trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d02c4f3",
   "metadata": {},
   "source": [
    "# Using n-gram models to predict whether a document is human or GPT generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24b22d2",
   "metadata": {},
   "source": [
    "Find bigram probabilities \n",
    "- classify each document using bigrams\n",
    "- classify each document using trigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42c8afaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram accuracy: 0.9595741141241058\n"
     ]
    }
   ],
   "source": [
    "# BIGRAM\n",
    "\n",
    "# go through each document in the human and then in the gpt TEST sets\n",
    "# for each document, calculate the probability of it being human generated, using human counts and P(human) \n",
    "# and then ALSO calculate the probability of it being gpt generated using gpt counts and P(GPT)\n",
    "# classify based on higher probability \n",
    "\n",
    "p_hum = 35742/54100 \n",
    "p_gpt = 18358/54100\n",
    "\n",
    "def classify_bi(document):\n",
    "    pi_hum = 0\n",
    "    pi_gpt = 0\n",
    "    \n",
    "    for i in range(len(document)):\n",
    "        if document[i] not in bigram_freq_hum:\n",
    "            bigram_freq_hum[document[i]] = 0\n",
    "        if document[i][0] not in unigram_freq_hum:\n",
    "            unigram_freq_hum[document[i][0]] = 0\n",
    "            \n",
    "        pi_hum += np.log((bigram_freq_hum[document[i]] + 1) / (unigram_freq_hum[document[i][0]] + vocab))\n",
    "        \n",
    "        if document[i] not in bigram_freq_gpt:\n",
    "            bigram_freq_gpt[document[i]] = 0\n",
    "        if document[i][0] not in unigram_freq_gpt:\n",
    "            unigram_freq_gpt[document[i][0]] = 0\n",
    "          \n",
    "        pi_gpt += np.log((bigram_freq_gpt[document[i]] + 1) / (unigram_freq_gpt[document[i][0]] + vocab))\n",
    "    \n",
    "    human_prob = np.log(p_hum) + pi_hum\n",
    "    gpt_prob = np.log(p_gpt) + pi_gpt\n",
    "        \n",
    "    if human_prob > gpt_prob:\n",
    "        return 0    # classified as human\n",
    "    elif gpt_prob > human_prob:\n",
    "        return 1    # classified as gpt\n",
    "    \n",
    "count = 0\n",
    "correct = 0\n",
    "    \n",
    "# For human set\n",
    "for key, value in test_bigrams_hum_dict.items():\n",
    "    count += 1\n",
    "    test_bigrams_hum_dict[key].append(('classification', classify_bi(value)))\n",
    "    if classify_bi(value) == 0:\n",
    "        correct += 1\n",
    "        \n",
    "# For gpt set\n",
    "for key, value in test_bigrams_gpt_dict.items():\n",
    "    count += 1\n",
    "    test_bigrams_gpt_dict[key].append(('classification', classify_bi(value)))\n",
    "    if classify_bi(value) == 1:\n",
    "        correct += 1\n",
    "\n",
    "print(\"Bigram accuracy:\", correct/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c4a4be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram accuracy: 0.9422725004159042\n"
     ]
    }
   ],
   "source": [
    "# TRIGRAM\n",
    "\n",
    "p_hum = 35742/54100 \n",
    "p_gpt = 18358/54100\n",
    "\n",
    "def classify_tri(document):\n",
    "    pi_hum = 0\n",
    "    pi_gpt = 0\n",
    "    \n",
    "    for i in range(len(document)):\n",
    "        if document[i] not in trigram_freq_hum:\n",
    "            trigram_freq_hum[document[i]] = 0\n",
    "        if document[i][0] not in bigram_freq_hum:\n",
    "            bigram_freq_hum[document[i][:2]] = 0\n",
    "            \n",
    "        pi_hum += np.log((trigram_freq_hum[document[i]] + 1) / (bigram_freq_hum[document[i][:2]] + vocab))\n",
    "        \n",
    "        if document[i] not in trigram_freq_gpt:\n",
    "            trigram_freq_gpt[document[i]] = 0\n",
    "        if document[i][0] not in bigram_freq_gpt:\n",
    "            bigram_freq_gpt[document[i][:2]] = 0\n",
    "          \n",
    "        pi_gpt += np.log((trigram_freq_gpt[document[i]] + 1) / (bigram_freq_gpt[document[i][:2]] + vocab))\n",
    "    \n",
    "    human_prob = np.log(p_hum) + pi_hum\n",
    "    gpt_prob = np.log(p_gpt) + pi_gpt\n",
    "        \n",
    "    if human_prob > gpt_prob:\n",
    "        return 0    # classified as human\n",
    "    elif gpt_prob > human_prob:\n",
    "        return 1    # classified as gpt\n",
    "    \n",
    "count2 = 0\n",
    "correct2 = 0\n",
    "    \n",
    "# For human set\n",
    "for key, value in test_trigrams_hum_dict.items():\n",
    "    count2 += 1\n",
    "    test_trigrams_hum_dict[key].append(('classification', classify_tri(value)))\n",
    "    if classify_tri(value) == 0:\n",
    "        correct2 += 1\n",
    "        \n",
    "# For gpt set\n",
    "for key, value in test_trigrams_gpt_dict.items():\n",
    "    count2 += 1\n",
    "    test_trigrams_gpt_dict[key].append(('classification', classify_tri(value)))\n",
    "    if classify_tri(value) == 1:\n",
    "        correct2 += 1\n",
    "        \n",
    "print(\"Trigram accuracy:\", correct2/count2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eddd69e",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ee6697a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generated from human bigrams:\n",
      "1 : [\"['ideas']\", \"['intersect']\", \"['because']\", \"['motion']\", \"['picture']\", \"['that']\", \"['poke']\", \"['a']\", \"['freshwater']\", \"['from']\", \"['sams']\", \"['wages']\", \"['management']\", \"['body']\", \"['occasionally']\", \"['he']\", \"['reveres']\", \"['could']\", \"['that']\", \"['causes']\"]\n",
      "2 : [\"['deutschland']\", \"['translates']\", \"['as']\", \"['web']\", \"['savvy']\", \"['desktop']\", \"['so']\", \"['6']\", \"['such']\", \"[',']\", \"['138']\", \"[',']\", \"['purpose']\", \"['<end>']\"]\n",
      "3 : [\"['ianal']\", \"['i']\", \"['entitled']\", \"['to']\", \"['transpose']\", \"['lyrics']\", \"['did']\", \"['fairly']\", \"['arduous']\", \"['terms']\", \"['despite']\", \"['this']\", \"['produces']\", \"['antiinflammatory']\", \"['medication']\", \"['dissolve']\", \"['or']\", \"['turning']\", \"['circles']\", \"['that']\"]\n",
      "4 : [\"['several']\", \"['exponentially']\", \"['increased']\", \"['regulation']\", \"['groups']\", \"['possibly']\", \"['blockades']\", \"['enforced']\", \"['when']\", \"['technical']\", \"['cause']\", \"['calluses']\", \"['and']\", \"['swells']\", \"['.']\", \"['<end>']\"]\n",
      "5 : [\"['administrators']\", \"[',']\", \"['enzymest']\", \"[',']\", \"['374']\", \"['.']\", \"['<end>']\"]\n"
     ]
    }
   ],
   "source": [
    "# Human Bigram\n",
    "\n",
    "print(\"Text generated from human bigrams:\")\n",
    "for j in range(5):\n",
    "    initial = '<start>'\n",
    "    sent = []\n",
    "\n",
    "    for k in range(20):\n",
    "        selected_bigrams = {}\n",
    "\n",
    "        # Iterate over the items in bigrams and filter based on the condition key[0] == initial\n",
    "        for key, value in bigram_freq_hum.items():\n",
    "            if key[0] == initial:\n",
    "                selected_bigrams[key] = value\n",
    "\n",
    "        new_dict = {}\n",
    "\n",
    "        # Iterate over the items in bigrams and calculate exp(count/temperature)\n",
    "        for bigram, count in selected_bigrams.items():\n",
    "            new_dict[bigram] = np.exp(count / 50)\n",
    "\n",
    "        # Initialize a variable for denominator and set it to zero\n",
    "        denom = 0\n",
    "\n",
    "        # Iterate over the values in new_dict and sum them to calculate the denominator\n",
    "        for value in new_dict.values():\n",
    "            denom += value\n",
    "\n",
    "        # Initialize an empty dictionary for final_dict\n",
    "        final_dict = {}\n",
    "\n",
    "        # Iterate over the items in new_dict and calculate value/denominator\n",
    "        for bigram, value in new_dict.items():\n",
    "            final_dict[bigram] = value / denom\n",
    "\n",
    "        second_elements = []\n",
    "        for key in final_dict.keys():\n",
    "            second_element = key[1]\n",
    "            second_elements.append(second_element) \n",
    "\n",
    "        initial = np.random.choice(second_elements, 1, p = list(final_dict.values()))   \n",
    "        sent.append(str(initial))\n",
    "\n",
    "        if initial == '<end>':\n",
    "            break\n",
    "    print(j+1, \":\", sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "01c015a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generated from GPT bigrams:\n",
      "1 : [\"['beverly']\", \"['hills']\", \"['very']\", \"['positive']\", \"['thing']\", \"['that']\", \"['reasonable']\", \"['grounds']\", \"['to']\", \"['urban']\", \"['cultures']\", \"['themselves']\", \"['that']\", \"['wo']\", \"['nt']\", \"['evaporate']\", \"['more']\", \"['values']\", \"['may']\", \"['simply']\"]\n",
      "2 : [\"['siamese']\", \"['twins']\", \"['will']\", \"['fit']\", \"['both']\", \"['great']\", \"['philosophers']\", \"['play']\", \"['quickly']\", \"['getting']\", \"['hepatitis']\", \"['depend']\", \"['on']\", \"['screen']\", \"['has']\", \"['nuclear']\", \"['explosions']\", \"['are']\", \"['encountered']\", \"['in']\"]\n",
      "3 : [\"['natural']\", \"['urges']\", \"['to']\", \"['sample']\", \"['and']\", \"['catastrophic']\", \"['consequences']\", \"['on']\", \"['chemical']\", \"['peels']\", \"[',']\", \"['pasteurized']\", \"[',']\", \"['502']\", \"['in']\", \"['boxes']\", \"['could']\", \"['escape']\", \"['keys']\", \"['available']\"]\n",
      "4 : [\"['hubert']\", \"['l']\", \"['sound']\", \"['1']\", \"['100']\", \"['41']\", \"['0']\", \"['1']\", \"['kilograms']\", \"['.']\", \"['<end>']\"]\n",
      "5 : [\"['second']\", \"['do']\", \"['i']\", \"['can']\", \"['authorize']\", \"['the']\", \"['fins']\", \"['help']\", \"['as']\", \"['download']\", \"['material']\", \"['.']\", \"['<end>']\"]\n"
     ]
    }
   ],
   "source": [
    "# GPT Bigram\n",
    "\n",
    "print(\"Text generated from GPT bigrams:\")\n",
    "for j in range(5):\n",
    "    initial = '<start>'\n",
    "    sent = []\n",
    "\n",
    "    for k in range(20):\n",
    "        selected_bigrams = {}\n",
    "\n",
    "        # Iterate over the items in bigrams and filter based on the condition key[0] == initial\n",
    "        for key, value in bigram_freq_gpt.items():\n",
    "            if key[0] == initial:\n",
    "                selected_bigrams[key] = value\n",
    "\n",
    "        new_dict = {}\n",
    "\n",
    "        # Iterate over the items in bigrams and calculate exp(count/temperature)\n",
    "        for bigram, count in selected_bigrams.items():\n",
    "            new_dict[bigram] = np.exp(count / 50)\n",
    "\n",
    "        # Initialize a variable for denominator and set it to zero\n",
    "        denom = 0\n",
    "\n",
    "        # Iterate over the values in new_dict and sum them to calculate the denominator\n",
    "        for value in new_dict.values():\n",
    "            denom += value\n",
    "\n",
    "        # Initialize an empty dictionary for final_dict\n",
    "        final_dict = {}\n",
    "\n",
    "        # Iterate over the items in new_dict and calculate value/denominator\n",
    "        for bigram, value in new_dict.items():\n",
    "            final_dict[bigram] = value / denom\n",
    "\n",
    "        second_elements = []\n",
    "        for key in final_dict.keys():\n",
    "            second_element = key[1]\n",
    "            second_elements.append(second_element) \n",
    "\n",
    "        initial = np.random.choice(second_elements, 1, p = list(final_dict.values()))   \n",
    "        sent.append(str(initial))\n",
    "\n",
    "        if initial == '<end>':\n",
    "            break\n",
    "    print(j+1, \":\", sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b2542d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8371abaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ebfe5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af9f762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1733c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526657f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
